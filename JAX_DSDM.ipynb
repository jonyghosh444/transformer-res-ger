{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonyghosh444/transformer-res-ger/blob/master/JAX_DSDM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Iz9kPUaD3sA"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade equinox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYUS6fzxO1Dy"
      },
      "outputs": [],
      "source": [
        "import nbformat\n",
        "\n",
        "path = \"/content/your_notebook.ipynb\"  # update with your notebook filename\n",
        "\n",
        "with open(path) as f:\n",
        "    nb = nbformat.read(f, as_version=nbformat.NO_CONVERT)\n",
        "\n",
        "# Remove problematic metadata\n",
        "if 'widgets' in nb['metadata']:\n",
        "    del nb['metadata']['widgets']\n",
        "\n",
        "# Optionally, strip all cell outputs\n",
        "for cell in nb.cells:\n",
        "    if 'outputs' in cell:\n",
        "        cell['outputs'] = []\n",
        "    if 'execution_count' in cell:\n",
        "        cell['execution_count'] = None\n",
        "\n",
        "# Save cleaned notebook\n",
        "cleaned_path = path.replace(\".ipynb\", \"_cleaned.ipynb\")\n",
        "with open(cleaned_path, 'w') as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(f\"Cleaned notebook saved to: {cleaned_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORjT-W-BD5Tg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBwKqssDEBM5"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5GMpn-1EBiZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, grad, vmap, pmap\n",
        "import equinox as eqx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import collections\n",
        "import time\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdxSRIWMEFJR"
      },
      "source": [
        "# MNIST DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQntfI_UEFeo"
      },
      "outputs": [],
      "source": [
        "def load_train_test_data():\n",
        "\n",
        "    train = MNIST(root=\"./train_data\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "    test = MNIST(root=\"./test_data\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "\n",
        "    train_data = []\n",
        "    print(\"Start Preparing Train Data\")\n",
        "    for image, label in tqdm(train):\n",
        "        image = jnp.array(image.view(-1).numpy())\n",
        "        train_data.append([image, label])\n",
        "\n",
        "    test_data = []\n",
        "    print(\"Start Preparing Test Data\")\n",
        "    for image, label in tqdm(test):\n",
        "        image = jnp.array(image.view(-1).numpy())\n",
        "        test_data.append([image, label])\n",
        "\n",
        "    train_labels = [label for _, label in train_data]\n",
        "    test_labels = [label for _, label in test_data]\n",
        "    print(\"Train label distribution:\", collections.Counter(train_labels))\n",
        "    print(\"Test label distribution:\", collections.Counter(test_labels))\n",
        "\n",
        "    return train_data, test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90WrLSQbEI4x"
      },
      "source": [
        "# Loading MNIST Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVh02ghkEKuR"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = load_train_test_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skPO4t3NENFh"
      },
      "source": [
        "# Splitting Train and Test Data based on Classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJUOKEPKEQDD"
      },
      "outputs": [],
      "source": [
        "def split_dataset_based_on_class(dataset, splitted_labels, num_data = \"all\"):\n",
        "\n",
        "    splitted_dataset_feat = []\n",
        "    splitted_dataset_labels = []\n",
        "\n",
        "    print('Start Making Splitted Dataset')\n",
        "    for sub_labels in tqdm(splitted_labels):\n",
        "        print(f\"Creating Dataset for label {sub_labels}\")\n",
        "        sub_dataset_feat = []\n",
        "        sub_dataset_labels = []\n",
        "        for input, label in dataset:\n",
        "            if num_data != \"all\" and len(sub_dataset_feat) > num_data:\n",
        "                break\n",
        "            if label in sub_labels:\n",
        "\n",
        "                sub_dataset_feat.append(input)\n",
        "                sub_dataset_labels.append(label)\n",
        "\n",
        "        splitted_dataset_feat.append(jnp.array(sub_dataset_feat))\n",
        "        splitted_dataset_labels.append(jnp.array(sub_dataset_labels))\n",
        "\n",
        "    return splitted_dataset_feat, splitted_dataset_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5UH1T2yESoS"
      },
      "outputs": [],
      "source": [
        "# You can give num_data = \"all\" or any integer numerical value upto 60000 for train\n",
        "splitted_train_feat, splitted_train_targets = split_dataset_based_on_class(train_data, [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]], num_data = \"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSzFRx5pEUWi"
      },
      "outputs": [],
      "source": [
        "# You can give num_data = \"all\" or any integer numerical value upto 10000 for test\n",
        "splitted_test_feat, splitted_test_targets = split_dataset_based_on_class(test_data, [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]], num_data = \"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEYM8R_QEdSS"
      },
      "source": [
        "# Local Outlier Factor coded in JAX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiONa_RNEZCy"
      },
      "outputs": [],
      "source": [
        "class LocalOutlierFactorJAX(eqx.Module):\n",
        "    k: int\n",
        "\n",
        "    def __init__(self, n_neighbors):\n",
        "\n",
        "        self.k = n_neighbors\n",
        "\n",
        "    def _pairwise_distances(self, X):\n",
        "        diffs = jnp.expand_dims(X, 1) - jnp.expand_dims(X, 0)\n",
        "        return jnp.sqrt(jnp.sum(diffs ** 2, axis=-1))\n",
        "\n",
        "    def _k_neighbors(self, dists):\n",
        "        n = dists.shape[0]\n",
        "        dists_no_self = dists + jnp.eye(n) * 1e10\n",
        "        return jnp.argsort(dists_no_self, axis=1)[:, :self.k]\n",
        "\n",
        "    def _reachability_distance(self, dists, neighbors, k_distances):\n",
        "        def single_point_reach(i):\n",
        "            nbrs = neighbors[i]\n",
        "            reach_d = jnp.maximum(k_distances[nbrs], dists[i, nbrs])\n",
        "            return jnp.mean(reach_d)\n",
        "        return jax.vmap(single_point_reach)(jnp.arange(dists.shape[0]))\n",
        "\n",
        "    def _local_reachability_density(self, reach_dists):\n",
        "        return 1.0 / reach_dists\n",
        "\n",
        "    def _lof_score(self, lrd, neighbors):\n",
        "        def single_lof(i):\n",
        "            nbrs = neighbors[i]\n",
        "            return jnp.sum(lrd[nbrs]) / (self.k * lrd[i])\n",
        "        return jax.vmap(single_lof)(jnp.arange(len(lrd)))\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, X):\n",
        "        dists = self._pairwise_distances(X)\n",
        "        neighbors = self._k_neighbors(dists)\n",
        "        k_dists = jnp.take_along_axis(dists, neighbors, axis=1)[:, -1]\n",
        "        reach_d = self._reachability_distance(dists, neighbors, k_dists)\n",
        "        lrd = self._local_reachability_density(reach_d)\n",
        "        lof = self._lof_score(lrd, neighbors)\n",
        "        return lof"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP8w1-YmEigr"
      },
      "source": [
        "# Naive Pruning Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To7jhcg3EbBi"
      },
      "outputs": [],
      "source": [
        "import equinox as eqx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import functools\n",
        "class NaivePruning(eqx.Module):\n",
        "    Q: int = eqx.static_field()\n",
        "    n_neighbors : int = eqx.static_field()\n",
        "    K : int = eqx.static_field()\n",
        "    N_prune : int = eqx.static_field()\n",
        "\n",
        "    def __init__(self, Q, K, n_neighbors = 20):\n",
        "        self.Q = Q\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.K = K\n",
        "        self.N_prune = K - Q\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def __call__(self, A, C):\n",
        "        #N_prune = K - self.Q\n",
        "        address = A[:self.K]\n",
        "        # Assume LocalOutlierFactorJAX works like a function\n",
        "        clf = LocalOutlierFactorJAX(n_neighbors=self.n_neighbors)\n",
        "        lof_scores = clf(address)\n",
        "\n",
        "        indices_to_include = jnp.argsort(-lof_scores)[:self.N_prune]\n",
        "        new_A = A[indices_to_include]\n",
        "        new_C = C[indices_to_include]\n",
        "\n",
        "        # Padding if needed\n",
        "        pad_A = A.shape[0] - new_A.shape[0]\n",
        "        pad_C = C.shape[0] - new_C.shape[0]\n",
        "        padding_A = jnp.zeros((pad_A, A.shape[1]))\n",
        "        padding_C = jnp.zeros((pad_C, C.shape[1]))\n",
        "        new_A = jnp.vstack([new_A, padding_A])\n",
        "        new_C = jnp.vstack([new_C, padding_C])\n",
        "\n",
        "        new_K = self.Q\n",
        "        return new_A, new_C, new_K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUswGdxdErDb"
      },
      "source": [
        "# Balance Pruning Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAHHJui1Ek1j"
      },
      "outputs": [],
      "source": [
        "class BalancePruning(eqx.Module):\n",
        "    Q: int = eqx.static_field()\n",
        "    buffer: int = eqx.static_field()\n",
        "    n_class: int = eqx.static_field()\n",
        "    n_neighbors: int = eqx.static_field()\n",
        "\n",
        "    def __init__(self, Q, buffer, n_class, n_neighbors=20):\n",
        "        self.Q = Q\n",
        "        self.buffer = buffer\n",
        "        self.n_class = n_class\n",
        "        self.n_neighbors = n_neighbors\n",
        "\n",
        "    def __call__(self, A, C, K):\n",
        "        return self._prune(A, C, K)\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def _prune(self, A, C, K):\n",
        "        N_prune = K - self.Q\n",
        "        mean_prune = N_prune // self.n_class\n",
        "\n",
        "        # Get LOF scores and sort indices\n",
        "        clf = LocalOutlierFactorJAX(n_neighbors=self.n_neighbors)\n",
        "        lof_scores = clf(A)\n",
        "        indices_to_include = jnp.argsort(lof_scores)\n",
        "\n",
        "        # Get class labels\n",
        "        classes = jnp.argmax(C, axis=1)\n",
        "\n",
        "        # Count how many to keep per class\n",
        "        class_counts = jnp.bincount(classes, length=self.n_class)\n",
        "\n",
        "        class_keep_counts = jnp.where(class_counts >= mean_prune, class_counts - mean_prune, class_counts)  # how many to keep\n",
        "\n",
        "\n",
        "        # Initialize outputs\n",
        "        new_A = jnp.zeros_like(A)\n",
        "        new_C = jnp.zeros_like(C)\n",
        "        counts = jnp.zeros((self.n_class,), dtype=jnp.int32)\n",
        "\n",
        "        def insert_data(carry, idx):\n",
        "            new_A, new_C, counts = carry\n",
        "            current_class = classes[idx].astype(jnp.int32)\n",
        "            keep_limit = class_keep_counts[current_class]\n",
        "\n",
        "            def place():\n",
        "                current_idx = jnp.sum(counts).astype(jnp.int32)\n",
        "                new_A_ = new_A.at[current_idx].set(A[idx])\n",
        "                new_C_ = new_C.at[current_idx].set(C[idx])\n",
        "                counts_ = counts.at[current_class].add(1)\n",
        "                return new_A_, new_C_, counts_\n",
        "\n",
        "            def skip():\n",
        "                return new_A, new_C, counts\n",
        "\n",
        "            cond = counts[current_class] < keep_limit\n",
        "            return jax.lax.cond(cond, place, skip), None\n",
        "\n",
        "        (new_A, new_C, counts), _ = jax.lax.scan(insert_data, (new_A, new_C, counts), indices_to_include)\n",
        "\n",
        "        # Total new size\n",
        "        new_K = jnp.sum(class_keep_counts).astype(jnp.int32)\n",
        "\n",
        "        return new_A, new_C, new_K, mean_prune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHxnfPt2Et-z"
      },
      "source": [
        "# Slicer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDFiY6tHEwBz"
      },
      "outputs": [],
      "source": [
        "class Slice(eqx.Module):\n",
        "\n",
        "    num_rows : int = eqx.static_field()\n",
        "\n",
        "    def __init__(self, num_rows):\n",
        "        self.num_rows = num_rows\n",
        "\n",
        "    def __call__(self, X):\n",
        "        return X[:self.num_rows]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiX9zAEfEzjj"
      },
      "source": [
        "# DSDM IN JAX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twz5NE4IE2ac"
      },
      "outputs": [],
      "source": [
        "class FASTER_DSDM(eqx.Module):\n",
        "    Address: jnp.ndarray\n",
        "    Content: jnp.ndarray\n",
        "    n_feat: int = eqx.static_field()\n",
        "    n_class: int = eqx.static_field()\n",
        "    RT: float\n",
        "    K: int\n",
        "    Q: int = eqx.static_field()\n",
        "    buffer: int = eqx.static_field()\n",
        "    beta: float\n",
        "    Lambda : float\n",
        "    Lambda_RT: float\n",
        "    prune_method: int = eqx.static_field()\n",
        "    n_neighbors: int = eqx.static_field()\n",
        "    contamination: float = eqx.static_field()\n",
        "\n",
        "    def __init__(self, RT=0, Q=100, buffer = 100, beta=0.022, Lambda=0.01, Lambda_RT=0.01,\n",
        "                 n_feat=784, n_class=10, prune_method=0, n_neighbors=1000, contamination=0.1):\n",
        "        self.n_feat = n_feat\n",
        "        self.n_class = n_class\n",
        "        self.Q = Q\n",
        "        self.buffer = buffer\n",
        "        self.Address = jnp.zeros((self.Q + self.buffer, self.n_feat))\n",
        "        self.Content = jnp.zeros((self.Q + self.buffer, self.n_class))\n",
        "        self.beta = beta\n",
        "        self.RT = RT\n",
        "        self.K = 0\n",
        "        self.Lambda = Lambda\n",
        "        self.Lambda_RT = Lambda_RT\n",
        "        self.prune_method = prune_method\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.contamination = contamination\n",
        "\n",
        "\n",
        "    def prune(self, address, content, K, RT):\n",
        "        naive_pruning = NaivePruning(\n",
        "            Q=self.Q,\n",
        "            K = K,\n",
        "            #buffer=self.buffer,\n",
        "            #n_class=self.n_class,\n",
        "            n_neighbors=self.n_neighbors,\n",
        "        )\n",
        "        #new_address, new_content, new_K, mean_prune = naive_pruning(address, content, K)\n",
        "        new_address, new_content, new_K = naive_pruning(address, content)\n",
        "        #print(mean_prune)\n",
        "        new_RT = RT + 0.0  # dummy update\n",
        "\n",
        "        return eqx.tree_at(\n",
        "            lambda model: (model.Address, model.Content, model.K, model.RT),\n",
        "            self,\n",
        "            (new_address, new_content, new_K, new_RT)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def return_same_model(self, address, content, K, RT):\n",
        "        return eqx.tree_at(lambda model : (model.Address, model.Content, model.K, model.RT),\n",
        "                           self,\n",
        "                           (address, content, K, RT))\n",
        "\n",
        "    #Calculating raw distance\n",
        "    def calculate_distance(self, x, y):\n",
        "        return x - y\n",
        "    #Calculating Eucledian Norm\n",
        "    def calculate_norm(self, distance):\n",
        "        return jnp.linalg.norm(distance, ord=2)\n",
        "\n",
        "    #Intiating Address and Content Matrix for the first Datapoint\n",
        "    def initiate_address_and_content(self, input_x_and_target_y):\n",
        "        input_x, target_y = input_x_and_target_y\n",
        "        new_address = self.Address.at[self.K].set(input_x)\n",
        "        one_hot_encoded_target = jax.nn.one_hot(target_y, num_classes=self.n_class)\n",
        "        new_content = self.Content.at[self.K].set(one_hot_encoded_target)\n",
        "        new_K = jnp.array(self.K + 1, dtype=jnp.int32)\n",
        "        new_RT = self.RT + 0.0\n",
        "        return new_address, new_content, new_K, new_RT\n",
        "\n",
        "    #Adding new node to the Address and Content Matrix if minimum distance of the datapoint is greater than RT(Recursive Temperature) Parameter\n",
        "    def add_new_node(self, input_x_and_target_y, min_BMU_distance):\n",
        "        input_x, target_y = input_x_and_target_y\n",
        "        new_address = self.Address.at[self.K].set(input_x)\n",
        "        one_hot_encoded_target = jax.nn.one_hot(target_y, num_classes=self.n_class)\n",
        "        new_content = self.Content.at[self.K].set(one_hot_encoded_target)\n",
        "        new_K = jnp.array(self.K + 1, dtype=jnp.int32)\n",
        "        #new_RT = self.Lambda_RT * self.RT + (1 - self.Lambda_RT) * min_BMU_distance\n",
        "        new_RT = (1 - self.Lambda_RT) * self.RT + self.Lambda_RT * min_BMU_distance\n",
        "        return new_address, new_content, new_K, new_RT\n",
        "\n",
        "    # Modifying Existing nodes in Address and Content Matrix using soft norm\n",
        "    def modify_existing_nodes(self, input_x_and_target_y, min_BMU_distance, all_BMU_distances):\n",
        "        input_x, target_y = input_x_and_target_y\n",
        "        soft_norm = jax.nn.softmax(-all_BMU_distances / self.beta, axis=-1)\n",
        "        soft_norm_reshaped = jnp.expand_dims(soft_norm, axis=-1)\n",
        "        address_diff = jax.vmap(self.calculate_distance, in_axes = (None, 0))(input_x, self.Address)\n",
        "\n",
        "        new_address = self.Address + self.Lambda * soft_norm_reshaped * (address_diff)\n",
        "        one_hot_encoded_target = jax.nn.one_hot(target_y, num_classes=self.n_class)\n",
        "        content_diff = jax.vmap(self.calculate_distance, in_axes = (None, 0))(one_hot_encoded_target, self.Content)\n",
        "        new_content = self.Content + self.Lambda * soft_norm_reshaped * content_diff\n",
        "        new_K = jnp.array(self.K, dtype=jnp.int32)\n",
        "        new_RT = (1 - self.Lambda_RT) * self.RT + self.Lambda_RT * min_BMU_distance\n",
        "        return new_address, new_content, new_K, new_RT\n",
        "\n",
        "    # Calculaing BMU distances of the datapoint from all the nodes in Address Matrix and Deciding whether to add a new node or modify the existing nodes\n",
        "    def add_or_modify_node(self, input_x_and_target_y):\n",
        "        input_x, target_y = input_x_and_target_y\n",
        "        distances = jax.vmap(self.calculate_distance, in_axes=(None, 0))(input_x, self.Address)\n",
        "        all_BMU_distances = jax.vmap(self.calculate_norm, in_axes=0)(distances)\n",
        "        min_BMU_distance = jnp.min(all_BMU_distances)\n",
        "\n",
        "        new_address, new_content, new_K, new_RT = jax.lax.cond(\n",
        "            abs(min_BMU_distance) > self.RT,\n",
        "            lambda: self.add_new_node(input_x_and_target_y, min_BMU_distance),\n",
        "            lambda: self.modify_existing_nodes(input_x_and_target_y, min_BMU_distance, all_BMU_distances)\n",
        "        )\n",
        "\n",
        "        return new_address, new_content, new_K, new_RT\n",
        "\n",
        "    # Forward pass is initiating the Content and Address Matrix if K(number of current Datapoints) == 0 else it is calling add_or_modify_function\n",
        "    def forward(self, input_x_and_target_y):\n",
        "        new_address, new_content, new_K, new_RT = jax.lax.cond(\n",
        "            self.K == 0,\n",
        "            lambda: self.initiate_address_and_content(input_x_and_target_y),\n",
        "            lambda: self.add_or_modify_node(input_x_and_target_y)\n",
        "        )\n",
        "        return eqx.tree_at(\n",
        "            lambda model: (model.Address, model.Content, model.K, model.RT),\n",
        "            self,\n",
        "            (new_address, new_content, new_K, new_RT)\n",
        "        ), None\n",
        "\n",
        "    # This is the Training Loop\n",
        "    @eqx.filter_jit\n",
        "    def train(self, inputs, targets):\n",
        "        #Doing A single Batch Calculation\n",
        "        def scan_fn(model, batch_x_and_y):\n",
        "            # Do Training for a single Datapoint in a Batch\n",
        "            def inner_scan_fn(m, x_and_y):\n",
        "                new_model, _ = m.forward(x_and_y)\n",
        "\n",
        "                return new_model, None\n",
        "\n",
        "            new_model, _ = jax.lax.scan(inner_scan_fn, model, batch_x_and_y)\n",
        "\n",
        "            return new_model, None\n",
        "\n",
        "        new_model, _ = jax.lax.scan(scan_fn, self, (inputs, targets))\n",
        "        return new_model\n",
        "\n",
        "    #This is the Inference Loop predicting the class for each test datapoint in batch\n",
        "    def inference(self, inputs, K):\n",
        "        # Convert to static int for eqx.static_field\n",
        "        K_int = int(K) if isinstance(K, jax.Array) else K\n",
        "\n",
        "        address_slicer = Slice(num_rows=K_int)\n",
        "        content_slicer = Slice(num_rows=K_int)\n",
        "\n",
        "        @eqx.filter_jit\n",
        "        def _jit_infer(self, inputs, address_slicer, content_slicer):\n",
        "            sliced_address = address_slicer(self.Address)\n",
        "            sliced_content = content_slicer(self.Content)\n",
        "\n",
        "            def batch_infer(carry, batch_x):\n",
        "                def single_infer(input_x):\n",
        "                    distances = jax.vmap(self.calculate_distance, in_axes=(None, 0))(input_x, sliced_address)\n",
        "                    norm = jax.vmap(self.calculate_norm, in_axes=0)(distances)\n",
        "                    soft_norm = jax.nn.softmax(-norm / self.beta, axis=-1)\n",
        "                    return jnp.argmax(jnp.matmul(soft_norm, sliced_content))\n",
        "\n",
        "                predictions = jax.vmap(single_infer)(batch_x)\n",
        "                return predictions, predictions\n",
        "\n",
        "            init_carry = jnp.zeros((inputs.shape[1],), dtype=jnp.int32)\n",
        "            predictions, _ = jax.lax.scan(batch_infer, init_carry, inputs)\n",
        "            return predictions\n",
        "\n",
        "        return _jit_infer(self, inputs, address_slicer, content_slicer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn3zIOYcE49M"
      },
      "source": [
        "# Training MNIST Raw pixels with pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zemcyLYE7Gs"
      },
      "outputs": [],
      "source": [
        "dsdm = FASTER_DSDM(RT=0, Q=5000, buffer = 12000, beta=0.5, Lambda=0.0022, Lambda_RT=0.0025,\n",
        "                 n_feat=784, n_class=10, prune_method=0, n_neighbors=20, contamination=0)\n",
        "\n",
        "\n",
        "class_seen = []\n",
        "train_data_seen = []\n",
        "test_data_seen = []\n",
        "start = time.time()\n",
        "print(\"DATASET : MNIST\")\n",
        "for input_x, target_y, test_x, test_y in tqdm(list(zip(splitted_train_feat, splitted_train_targets, splitted_test_feat, splitted_test_targets))):\n",
        "    unique_classes = list(set(target_y.tolist()))\n",
        "    class_seen += unique_classes\n",
        "    input_x = jnp.expand_dims(input_x, axis = 0)\n",
        "    target_y = jnp.expand_dims(target_y, axis = 0)\n",
        "    test_x = jnp.expand_dims(test_x, axis = 0)\n",
        "    test_y = jnp.expand_dims(test_y, axis = 0)\n",
        "    train_data_seen.append((input_x, target_y))\n",
        "    test_data_seen.append((test_x, test_y))\n",
        "    dsdm = dsdm.train(input_x, target_y)\n",
        "\n",
        "    if dsdm.K > dsdm.Q:\n",
        "        print(\"Nodes before pruning, \", dsdm.K)\n",
        "        dsdm = dsdm.prune(dsdm.Address, dsdm.Content, dsdm.K, dsdm.RT)\n",
        "        print(\"Nodes after pruning \", dsdm.K)\n",
        "\n",
        "    train_accuracy = 0\n",
        "    train_total_data = 0\n",
        "    for train_x, train_y in train_data_seen:\n",
        "        train_pred = dsdm.inference(train_x, dsdm.K)\n",
        "        train_accuracy += (train_pred == train_y).sum().item()\n",
        "        train_total_data += train_y.shape[1]\n",
        "\n",
        "    print(f\"Train Accuracy:{(train_accuracy / train_total_data) * 100} % after seeing class {class_seen}\")\n",
        "\n",
        "    test_accuracy = 0\n",
        "    test_total_data = 0\n",
        "    for test_x, test_y in test_data_seen:\n",
        "        test_pred = dsdm.inference(test_x, dsdm.K)\n",
        "        test_accuracy += (test_pred == test_y).sum().item()\n",
        "        test_total_data += test_y.shape[1]\n",
        "\n",
        "    print(f\"Test Accuracy:{(test_accuracy / test_total_data * 100)} % after seeing class {class_seen}\")\n",
        "    print(\"===========================================================================================\")\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Total Training Time required : {end - start}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mz5vSEqE-MM"
      },
      "source": [
        "# Training on MNIST raw pixels without pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4M81Nn0FALE"
      },
      "outputs": [],
      "source": [
        "dsdm = FASTER_DSDM(RT=0, Q=30000, buffer = 5000, beta=0.5, Lambda=0.0022, Lambda_RT=0.0025,\n",
        "                 n_feat=784, n_class=10, prune_method=0, n_neighbors=20, contamination=0)\n",
        "\n",
        "\n",
        "class_seen = []\n",
        "train_data_seen = []\n",
        "test_data_seen = []\n",
        "start = time.time()\n",
        "print(\"DATASET : MNIST\")\n",
        "for input_x, target_y, test_x, test_y in tqdm(list(zip(splitted_train_feat, splitted_train_targets, splitted_test_feat, splitted_test_targets))):\n",
        "    unique_classes = list(set(target_y.tolist()))\n",
        "    class_seen += unique_classes\n",
        "    input_x = jnp.expand_dims(input_x, axis = 0)\n",
        "    target_y = jnp.expand_dims(target_y, axis = 0)\n",
        "    test_x = jnp.expand_dims(test_x, axis = 0)\n",
        "    test_y = jnp.expand_dims(test_y, axis = 0)\n",
        "    train_data_seen.append((input_x, target_y))\n",
        "    test_data_seen.append((test_x, test_y))\n",
        "    dsdm = dsdm.train(input_x, target_y)\n",
        "\n",
        "\n",
        "    print(\"Current Nodes : \", dsdm.K)\n",
        "\n",
        "    train_accuracy = 0\n",
        "    train_total_data = 0\n",
        "    for train_x, train_y in train_data_seen:\n",
        "        train_pred = dsdm.inference(train_x, dsdm.K)\n",
        "        train_accuracy += (train_pred == train_y).sum().item()\n",
        "        train_total_data += train_y.shape[1]\n",
        "\n",
        "    print(f\"Train Accuracy:{(train_accuracy / train_total_data) * 100} % after seeing class {class_seen}\")\n",
        "\n",
        "    test_accuracy = 0\n",
        "    test_total_data = 0\n",
        "    for test_x, test_y in test_data_seen:\n",
        "        test_pred = dsdm.inference(test_x, dsdm.K)\n",
        "        test_accuracy += (test_pred == test_y).sum().item()\n",
        "        test_total_data += test_y.shape[1]\n",
        "\n",
        "    print(f\"Test Accuracy:{(test_accuracy / test_total_data * 100)} % after seeing class {class_seen}\")\n",
        "    print(\"===========================================================================================\")\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Total Training Time required : {end - start}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTWnTBsTFE11"
      },
      "source": [
        "train by using Encoded Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQckowoFGxd"
      },
      "source": [
        "CORE 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDQ4JeBzFHO1"
      },
      "source": [
        "# Loading Core50_resnet18_224.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwWSHesjFw0_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.load(\"/content/drive/MyDrive/DSDM/dataset/Core50_resnet18_224.npz\")\n",
        "train_x = data[\"traindata\"]\n",
        "train_y = data[\"trainlabel\"]\n",
        "test_x  = data[\"testdata\"]\n",
        "test_y  = data[\"label_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMuWvVNFyMw"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "train_x = jnp.array(train_x)\n",
        "train_y = jnp.array(train_y)\n",
        "test_x  = jnp.array(test_x)\n",
        "test_y  = jnp.array(test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhUir21JFzpX"
      },
      "outputs": [],
      "source": [
        "def split_dataset_by_class(X, Y, class_splits):\n",
        "    split_x = []\n",
        "    split_y = []\n",
        "    for cls_group in class_splits:\n",
        "        idxs = jnp.isin(Y, jnp.array(cls_group))\n",
        "        split_x.append(X[idxs])\n",
        "        split_y.append(Y[idxs])\n",
        "    return split_x, split_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zp4O_F8F1SX"
      },
      "outputs": [],
      "source": [
        "class_splits = [list(range(i, i+10)) for i in range(0, 50, 10)]\n",
        "splitted_train_x, splitted_train_y = split_dataset_by_class(train_x, train_y, class_splits)\n",
        "splitted_test_x, splitted_test_y = split_dataset_by_class(test_x, test_y, class_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCgmG2-YF22Q"
      },
      "source": [
        "Train DSDM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkdUTXMLF4cQ"
      },
      "outputs": [],
      "source": [
        "dsdm = FASTER_DSDM(\n",
        "    RT=0,\n",
        "    Q=5000,\n",
        "    buffer = 15000,\n",
        "    beta=0.5,\n",
        "    Lambda=0.0022,\n",
        "    Lambda_RT=0.0025,\n",
        "    n_feat=512,\n",
        "    n_class=50,\n",
        "    prune_method=0,\n",
        "    n_neighbors=20,\n",
        "    contamination=0.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgqnbEQ7F6jY"
      },
      "outputs": [],
      "source": [
        "train_data_seen = []\n",
        "test_data_seen = []\n",
        "class_seen = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RKqgZk3F8fY"
      },
      "source": [
        "# Training on Core 50 Resnet18 Encoded with pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bLjsUsbF-e5"
      },
      "outputs": [],
      "source": [
        "for train_x_batch, train_y_batch, test_x_batch, test_y_batch in tqdm(\n",
        "    list(zip(splitted_train_x, splitted_train_y, splitted_test_x, splitted_test_y)),\n",
        "    desc=\"Class-incremental Training\"\n",
        "):\n",
        "\n",
        "    unique_classes = list(set(train_y_batch.tolist()))\n",
        "    class_seen += unique_classes\n",
        "\n",
        "\n",
        "    train_x_batch = jnp.expand_dims(train_x_batch, axis=0)\n",
        "    train_y_batch = jnp.expand_dims(train_y_batch, axis=0)\n",
        "    test_x_batch = jnp.expand_dims(test_x_batch, axis=0)\n",
        "    test_y_batch = jnp.expand_dims(test_y_batch, axis=0)\n",
        "\n",
        "    train_data_seen.append((train_x_batch, train_y_batch))\n",
        "    test_data_seen.append((test_x_batch, test_y_batch))\n",
        "\n",
        "\n",
        "    dsdm = dsdm.train(train_x_batch, train_y_batch)\n",
        "\n",
        "\n",
        "    if dsdm.K > dsdm.Q:\n",
        "        print(\"Nodes before pruning : \", dsdm.K)\n",
        "        dsdm = dsdm.prune(dsdm.Address, dsdm.Content, dsdm.K, dsdm.RT)\n",
        "        print(\"Nodes after pruning : \", dsdm.K)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for tx, ty in train_data_seen:\n",
        "        preds = dsdm.inference(tx, dsdm.K)\n",
        "        correct += (preds == ty).sum().item()\n",
        "        total += ty.shape[1]\n",
        "    train_acc = (correct / total) * 100\n",
        "    print(f\"Train Accuracy: {train_acc:.2f}% after classes {class_seen}\")\n",
        "\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for tx, ty in test_data_seen:\n",
        "        preds = dsdm.inference(tx, dsdm.K)\n",
        "        correct += (preds == ty).sum().item()\n",
        "        total += ty.shape[1]\n",
        "    test_acc = (correct / total) * 100\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}% after classes {class_seen}\")\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS5tLjUsGBeb"
      },
      "source": [
        "# Training on Core 50 Resnet18 Encoded without pruning,  final nodes amount for without pruing case, it's so much"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MQjqPsqGClJ"
      },
      "outputs": [],
      "source": [
        "dsdm = FASTER_DSDM(\n",
        "    RT=0,\n",
        "    Q=80000,\n",
        "    buffer = 15000,\n",
        "    beta=0.5,\n",
        "    Lambda=0.0022,\n",
        "    Lambda_RT=0.0025,\n",
        "    n_feat=512,\n",
        "    n_class=50,\n",
        "    prune_method=0,\n",
        "    n_neighbors=20,\n",
        "    contamination=0.0,\n",
        ")\n",
        "\n",
        "train_data_seen = []\n",
        "test_data_seen = []\n",
        "class_seen = []\n",
        "\n",
        "for train_x_batch, train_y_batch, test_x_batch, test_y_batch in tqdm(\n",
        "    list(zip(splitted_train_x, splitted_train_y, splitted_test_x, splitted_test_y)),\n",
        "    desc=\"Class-incremental Training\"\n",
        "):\n",
        "\n",
        "    unique_classes = list(set(train_y_batch.tolist()))\n",
        "    class_seen += unique_classes\n",
        "\n",
        "\n",
        "    train_x_batch = jnp.expand_dims(train_x_batch, axis=0)\n",
        "    train_y_batch = jnp.expand_dims(train_y_batch, axis=0)\n",
        "    test_x_batch = jnp.expand_dims(test_x_batch, axis=0)\n",
        "    test_y_batch = jnp.expand_dims(test_y_batch, axis=0)\n",
        "\n",
        "    train_data_seen.append((train_x_batch, train_y_batch))\n",
        "    test_data_seen.append((test_x_batch, test_y_batch))\n",
        "\n",
        "\n",
        "    dsdm = dsdm.train(train_x_batch, train_y_batch)\n",
        "\n",
        "    \"\"\"\n",
        "    if dsdm.K > dsdm.Q:\n",
        "        print(\"Nodes before pruning : \", dsdm.K)\n",
        "        dsdm = dsdm.prune(dsdm.Address, dsdm.Content, dsdm.K, dsdm.RT)\n",
        "        print(\"Nodes after pruning : \", dsdm.K)\n",
        "    \"\"\"\n",
        "    print(\"Current Nodes : \", dsdm.K)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for tx, ty in train_data_seen:\n",
        "        preds = dsdm.inference(tx, dsdm.K)\n",
        "        correct += (preds == ty).sum().item()\n",
        "        total += ty.shape[1]\n",
        "    train_acc = (correct / total) * 100\n",
        "    print(f\"Train Accuracy: {train_acc:.2f}% after classes {class_seen}\")\n",
        "\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for tx, ty in test_data_seen:\n",
        "        preds = dsdm.inference(tx, dsdm.K)\n",
        "        correct += (preds == ty).sum().item()\n",
        "        total += ty.shape[1]\n",
        "    test_acc = (correct / total) * 100\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}% after classes {class_seen}\")\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE68VFzSGYyB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyN6XvnSYhxWpW5GD/QOYOSt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}